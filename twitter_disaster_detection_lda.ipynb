{
  "cells": [
    {
      "cell_type": "markdown",
      "source": "## Latent Dirichlet allocation",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "### Load Data",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "outputs": [
        {
          "data": {
            "text/plain": "   tweetid                                               text disaster_type  \\\n0    10001  @TheEllenShow Please check into Salt River hor...           NaN   \n1    10002  As for the hurricane, it\u0027s already category 1 ...     hurricane   \n2    10003  So it looks like my @SoundCloud profile shall ...           NaN   \n3    10004  @SushmaSwaraj Am sure background check of the ...           NaN   \n4    10005  Open forex detonation indicator is irretrievab...           NaN   \n\n   disaster  Unnamed: 4  \n0         0         NaN  \n1         1         NaN  \n2         0         NaN  \n3         0         NaN  \n4         0         NaN  ",
            "text/html": "\u003cdiv\u003e\n\u003cstyle scoped\u003e\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n\u003c/style\u003e\n\u003ctable border\u003d\"1\" class\u003d\"dataframe\"\u003e\n  \u003cthead\u003e\n    \u003ctr style\u003d\"text-align: right;\"\u003e\n      \u003cth\u003e\u003c/th\u003e\n      \u003cth\u003etweetid\u003c/th\u003e\n      \u003cth\u003etext\u003c/th\u003e\n      \u003cth\u003edisaster_type\u003c/th\u003e\n      \u003cth\u003edisaster\u003c/th\u003e\n      \u003cth\u003eUnnamed: 4\u003c/th\u003e\n    \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n    \u003ctr\u003e\n      \u003cth\u003e0\u003c/th\u003e\n      \u003ctd\u003e10001\u003c/td\u003e\n      \u003ctd\u003e@TheEllenShow Please check into Salt River hor...\u003c/td\u003e\n      \u003ctd\u003eNaN\u003c/td\u003e\n      \u003ctd\u003e0\u003c/td\u003e\n      \u003ctd\u003eNaN\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e1\u003c/th\u003e\n      \u003ctd\u003e10002\u003c/td\u003e\n      \u003ctd\u003eAs for the hurricane, it\u0027s already category 1 ...\u003c/td\u003e\n      \u003ctd\u003ehurricane\u003c/td\u003e\n      \u003ctd\u003e1\u003c/td\u003e\n      \u003ctd\u003eNaN\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e2\u003c/th\u003e\n      \u003ctd\u003e10003\u003c/td\u003e\n      \u003ctd\u003eSo it looks like my @SoundCloud profile shall ...\u003c/td\u003e\n      \u003ctd\u003eNaN\u003c/td\u003e\n      \u003ctd\u003e0\u003c/td\u003e\n      \u003ctd\u003eNaN\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e3\u003c/th\u003e\n      \u003ctd\u003e10004\u003c/td\u003e\n      \u003ctd\u003e@SushmaSwaraj Am sure background check of the ...\u003c/td\u003e\n      \u003ctd\u003eNaN\u003c/td\u003e\n      \u003ctd\u003e0\u003c/td\u003e\n      \u003ctd\u003eNaN\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e4\u003c/th\u003e\n      \u003ctd\u003e10005\u003c/td\u003e\n      \u003ctd\u003eOpen forex detonation indicator is irretrievab...\u003c/td\u003e\n      \u003ctd\u003eNaN\u003c/td\u003e\n      \u003ctd\u003e0\u003c/td\u003e\n      \u003ctd\u003eNaN\u003c/td\u003e\n    \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003c/div\u003e"
          },
          "metadata": {},
          "output_type": "execute_result",
          "execution_count": 10
        }
      ],
      "source": "import pandas as pd\ntrain_data_raw \u003d pd.read_csv(\"./data/train.csv\", encoding\u003d\"ISO-8859-1\") #utf-8 doesn\u0027t play nice\ntest_data_raw \u003d pd.read_csv(\"./data/test.csv\", encoding\u003d\"ISO-8859-1\")\ntrain_data_raw.head()",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "### Clean Corpus",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "themaine, soon, start, flooding, comment, section, come, brazil, comment, bc\nlittle, weak, as, earthquake\nooh, girl, actually, meeting, boy, im, devastated, congrats, winner, otrabaltimore\ngraceville, underground, car, park, flooding, rapidly, excyclone, debbie, dump, large, rainfall, se, queensland, httpstco3uju1n7ns1\nnext, man, upah, screw, this, im, tired, injury, happened, camp, cupcake, like, camp, cramp, break\ndying, debt, costly, survivor\noh, god, rosie, oûªdonnell, foster, screamed, name, fit, shock, awe\nfound, fish, skeleton, along, path, walk, probably, got, stuck, puddle, flood, stage, httpstcopafc9jk3ty\nafrojazz, ill, add, i, dont, even, know, talking, about\nð¿ñð, ñtropical, cyclone, debbie, cause, widespread, damage, north, queensland, ñ, httpstcodfvn3pb0zn\nmikeparractor, devastated, longer, emmerdale, best, character, much, give, superbactor, going, missed\near, gonna, explode, smh\ntwo, can, explode, wanted, drink, rest, kaldi, coffee, stout, httptcou6isxv2f3v, photo\nstay, safe, queenslanders, httpstco02xehcha49\nhit, tailor, fucking, made, double, play, kay, scream, robbed, hit\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "from nltk.corpus import stopwords\nfrom nltk.stem.wordnet import WordNetLemmatizer\nimport string\nstop \u003d set(stopwords.words(\u0027english\u0027))\nexclude \u003d set(string.punctuation)\nlemma \u003d WordNetLemmatizer()\ndef clean(doc):\n    stop_free \u003d \" \".join([i for i in doc.lower().split() if i not in stop])\n    punc_free \u003d \u0027\u0027.join(ch for ch in stop_free if ch not in exclude)\n    normalized \u003d \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n    return normalized\n\ndoc_clean \u003d [clean(doc).split() for doc in test_data_raw[\"text\"]]\nprint(\"\\n\".join(\", \".join(i) for i in doc_clean[:15]))",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "### Prepare Document-Term Matrix\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "[, (, 0, ,,  , 1, ), ,,  , (, 1, ,,  , 1, ), ,,  , (, 2, ,,  , 1, ), ,,  , (, 3, ,,  , 2, ), ,,  , (, 4, ,,  , 1, ), ,,  , (, 5, ,,  , 1, ), ,,  , (, 6, ,,  , 1, ), ,,  , (, 7, ,,  , 1, ), ,,  , (, 8, ,,  , 1, ), ]\n[, (, 9, ,,  , 1, ), ,,  , (, 1, 0, ,,  , 1, ), ,,  , (, 1, 1, ,,  , 1, ), ,,  , (, 1, 2, ,,  , 1, ), ]\n[, (, 1, 3, ,,  , 1, ), ,,  , (, 1, 4, ,,  , 1, ), ,,  , (, 1, 5, ,,  , 1, ), ,,  , (, 1, 6, ,,  , 1, ), ,,  , (, 1, 7, ,,  , 1, ), ,,  , (, 1, 8, ,,  , 1, ), ,,  , (, 1, 9, ,,  , 1, ), ,,  , (, 2, 0, ,,  , 1, ), ,,  , (, 2, 1, ,,  , 1, ), ,,  , (, 2, 2, ,,  , 1, ), ]\n[, (, 4, ,,  , 1, ), ,,  , (, 2, 3, ,,  , 1, ), ,,  , (, 2, 4, ,,  , 1, ), ,,  , (, 2, 5, ,,  , 1, ), ,,  , (, 2, 6, ,,  , 1, ), ,,  , (, 2, 7, ,,  , 1, ), ,,  , (, 2, 8, ,,  , 1, ), ,,  , (, 2, 9, ,,  , 1, ), ,,  , (, 3, 0, ,,  , 1, ), ,,  , (, 3, 1, ,,  , 1, ), ,,  , (, 3, 2, ,,  , 1, ), ,,  , (, 3, 3, ,,  , 1, ), ,,  , (, 3, 4, ,,  , 1, ), ,,  , (, 3, 5, ,,  , 1, ), ]\n[, (, 1, 8, ,,  , 1, ), ,,  , (, 3, 6, ,,  , 1, ), ,,  , (, 3, 7, ,,  , 2, ), ,,  , (, 3, 8, ,,  , 1, ), ,,  , (, 3, 9, ,,  , 1, ), ,,  , (, 4, 0, ,,  , 1, ), ,,  , (, 4, 1, ,,  , 1, ), ,,  , (, 4, 2, ,,  , 1, ), ,,  , (, 4, 3, ,,  , 1, ), ,,  , (, 4, 4, ,,  , 1, ), ,,  , (, 4, 5, ,,  , 1, ), ,,  , (, 4, 6, ,,  , 1, ), ,,  , (, 4, 7, ,,  , 1, ), ,,  , (, 4, 8, ,,  , 1, ), ]\n[, (, 4, 9, ,,  , 1, ), ,,  , (, 5, 0, ,,  , 1, ), ,,  , (, 5, 1, ,,  , 1, ), ,,  , (, 5, 2, ,,  , 1, ), ]\n[, (, 5, 3, ,,  , 1, ), ,,  , (, 5, 4, ,,  , 1, ), ,,  , (, 5, 5, ,,  , 1, ), ,,  , (, 5, 6, ,,  , 1, ), ,,  , (, 5, 7, ,,  , 1, ), ,,  , (, 5, 8, ,,  , 1, ), ,,  , (, 5, 9, ,,  , 1, ), ,,  , (, 6, 0, ,,  , 1, ), ,,  , (, 6, 1, ,,  , 1, ), ,,  , (, 6, 2, ,,  , 1, ), ]\n[, (, 6, 3, ,,  , 1, ), ,,  , (, 6, 4, ,,  , 1, ), ,,  , (, 6, 5, ,,  , 1, ), ,,  , (, 6, 6, ,,  , 1, ), ,,  , (, 6, 7, ,,  , 1, ), ,,  , (, 6, 8, ,,  , 1, ), ,,  , (, 6, 9, ,,  , 1, ), ,,  , (, 7, 0, ,,  , 1, ), ,,  , (, 7, 1, ,,  , 1, ), ,,  , (, 7, 2, ,,  , 1, ), ,,  , (, 7, 3, ,,  , 1, ), ,,  , (, 7, 4, ,,  , 1, ), ,,  , (, 7, 5, ,,  , 1, ), ]\n[, (, 7, 6, ,,  , 1, ), ,,  , (, 7, 7, ,,  , 1, ), ,,  , (, 7, 8, ,,  , 1, ), ,,  , (, 7, 9, ,,  , 1, ), ,,  , (, 8, 0, ,,  , 1, ), ,,  , (, 8, 1, ,,  , 1, ), ,,  , (, 8, 2, ,,  , 1, ), ,,  , (, 8, 3, ,,  , 1, ), ,,  , (, 8, 4, ,,  , 1, ), ]\n[, (, 2, 4, ,,  , 1, ), ,,  , (, 3, 1, ,,  , 1, ), ,,  , (, 8, 5, ,,  , 1, ), ,,  , (, 8, 6, ,,  , 1, ), ,,  , (, 8, 7, ,,  , 1, ), ,,  , (, 8, 8, ,,  , 1, ), ,,  , (, 8, 9, ,,  , 1, ), ,,  , (, 9, 0, ,,  , 1, ), ,,  , (, 9, 1, ,,  , 1, ), ,,  , (, 9, 2, ,,  , 1, ), ,,  , (, 9, 3, ,,  , 1, ), ]\n[, (, 1, 6, ,,  , 1, ), ,,  , (, 9, 4, ,,  , 1, ), ,,  , (, 9, 5, ,,  , 1, ), ,,  , (, 9, 6, ,,  , 1, ), ,,  , (, 9, 7, ,,  , 1, ), ,,  , (, 9, 8, ,,  , 1, ), ,,  , (, 9, 9, ,,  , 1, ), ,,  , (, 1, 0, 0, ,,  , 1, ), ,,  , (, 1, 0, 1, ,,  , 1, ), ,,  , (, 1, 0, 2, ,,  , 1, ), ,,  , (, 1, 0, 3, ,,  , 1, ), ]\n[, (, 1, 0, 4, ,,  , 1, ), ,,  , (, 1, 0, 5, ,,  , 1, ), ,,  , (, 1, 0, 6, ,,  , 1, ), ,,  , (, 1, 0, 7, ,,  , 1, ), ]\n[, (, 1, 0, 5, ,,  , 1, ), ,,  , (, 1, 0, 8, ,,  , 1, ), ,,  , (, 1, 0, 9, ,,  , 1, ), ,,  , (, 1, 1, 0, ,,  , 1, ), ,,  , (, 1, 1, 1, ,,  , 1, ), ,,  , (, 1, 1, 2, ,,  , 1, ), ,,  , (, 1, 1, 3, ,,  , 1, ), ,,  , (, 1, 1, 4, ,,  , 1, ), ,,  , (, 1, 1, 5, ,,  , 1, ), ,,  , (, 1, 1, 6, ,,  , 1, ), ,,  , (, 1, 1, 7, ,,  , 1, ), ]\n[, (, 1, 1, 8, ,,  , 1, ), ,,  , (, 1, 1, 9, ,,  , 1, ), ,,  , (, 1, 2, 0, ,,  , 1, ), ,,  , (, 1, 2, 1, ,,  , 1, ), ]\n[, (, 1, 2, 2, ,,  , 1, ), ,,  , (, 1, 2, 3, ,,  , 1, ), ,,  , (, 1, 2, 4, ,,  , 2, ), ,,  , (, 1, 2, 5, ,,  , 1, ), ,,  , (, 1, 2, 6, ,,  , 1, ), ,,  , (, 1, 2, 7, ,,  , 1, ), ,,  , (, 1, 2, 8, ,,  , 1, ), ,,  , (, 1, 2, 9, ,,  , 1, ), ,,  , (, 1, 3, 0, ,,  , 1, ), ]\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "from gensim import corpora\n\n# Creating the term dictionary of our corpus, where every unique term is assigned an index. \ndictionary \u003d corpora.Dictionary(doc_clean)\n\n# Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\ndoc_term_matrix \u003d [dictionary.doc2bow(doc) for doc in doc_clean]\nprint(\"\\n\".join(\", \".join(str(i)) for i in doc_term_matrix[:15]))\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "### Run LDA Model",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "Lda model trained in 83.69358396530151\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "### # Creating the object for LDA model using gensim library\nfrom gensim.models.ldamodel import LdaModel as Lda\nfrom time import time\n\n# Running and Training LDA model on the document term matrix.\nstart \u003d time()\nldaModel \u003d Lda(doc_term_matrix, num_topics\u003d2, id2word \u003d dictionary, passes\u003d50)\nend \u003d time()\n\nprint(f\"Lda model trained in {end - start}\")\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "(0, \u00270.005*\"new\" + 0.003*\"bag\" + 0.003*\"via\" + 0.003*\"amp\" + 0.003*\"body\"\u0027)\n(1, \u00270.008*\"earthquake\" + 0.007*\"like\" + 0.007*\"im\" + 0.006*\"debbie\" + 0.006*\"cyclone\"\u0027)\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "print(\"\\n\".join(str(i) for i in ldaModel.print_topics(num_topics\u003d2, num_words\u003d5)))",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    }
  ],
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "stem_cell": {
      "cell_type": "raw",
      "source": "",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}